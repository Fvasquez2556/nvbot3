# ü§ñ Instrucciones para GitHub Copilot - Training Pipeline NvBot3

## üîß Setup y Configuraci√≥n del Entorno

### Instrucci√≥n #1: Configuraci√≥n del Proyecto
```bash
# Copilot: Siempre usar el entorno virtual para todas las operaciones
# Crear estas instrucciones en un script setup.py

# 1. Crear entorno virtual
python -m venv nvbot3_env

# 2. Activar entorno (Windows)
nvbot3_env\Scripts\activate
# Activar entorno (Linux/Mac)  
source nvbot3_env/bin/activate

# 3. Upgrade pip
python -m pip install --upgrade pip

# 4. Instalar dependencias
pip install -r requirements.txt

# 5. Verificar instalaci√≥n
python -c "import numpy, pandas, sklearn, xgboost; print('All dependencies installed successfully')"
```

**IMPORTANTE para Copilot:** Todas las ejecuciones de Python deben usar el entorno virtual activado. Si abres nueva terminal, incluir siempre la activaci√≥n del entorno.

---

## üìä M√≥dulo 1: Historical Data Downloader

### Archivo: `scripts/download_historical_data.py`

**Objetivo:** Descargar 2+ a√±os de datos hist√≥ricos de Binance para entrenamiento

**Especificaciones t√©cnicas:**
- **S√≠mbolos:** BTCUSDT, ETHUSDT, BNBUSDT, ADAUSDT, SOLUSDT (expandir despu√©s)
- **Timeframes:** 5m, 15m, 1h, 4h, 1d
- **Per√≠odo:** Desde enero 2022 hasta presente
- **Formato:** Guardar como CSV en `data/raw/{symbol}_{timeframe}.csv`
- **Rate limiting:** Respetar l√≠mites de Binance API (m√°ximo 1200 requests/minuto)

**Estructura de datos requerida:**
```python
columns = ['timestamp', 'open', 'high', 'low', 'close', 'volume', 
           'close_time', 'quote_asset_volume', 'number_of_trades', 
           'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume']
```

**Funcionalidades requeridas:**
1. **Progress tracking:** Mostrar progreso de descarga por s√≠mbolo/timeframe
2. **Resume capability:** Poder continuar descarga si se interrumpe
3. **Data validation:** Verificar que no hay gaps mayores a timeframe * 2
4. **Error handling:** Retry autom√°tico con backoff exponencial
5. **Memory efficiency:** Procesar en chunks para evitar saturar RAM

**C√≥digo base solicitado:**
```python
import ccxt
import pandas as pd
import time
from datetime import datetime, timedelta
import os
from tqdm import tqdm

class HistoricalDataDownloader:
    def __init__(self):
        # Inicializar conexi√≥n Binance
        # Configurar rate limiting
        # Setup de directorios
        pass
    
    def download_symbol_timeframe(self, symbol, timeframe, start_date, end_date):
        # Implementar descarga por chunks
        # Manejar rate limiting
        # Validar datos descargados
        pass
    
    def download_all_data(self):
        # Iterar sobre todos los s√≠mbolos y timeframes
        # Mostrar progreso general
        # Manejar errores y reintentos
        pass
```

---

## üîç M√≥dulo 2: Data Validator

### Archivo: `src/data/data_validator.py`

**Objetivo:** Validar calidad de datos hist√≥ricos antes del entrenamiento

**Validaciones requeridas:**
1. **Completeness check:** Sin gaps > 2 per√≠odos consecutivos
2. **Price validity:** No cambios > 50% en un per√≠odo
3. **Volume validity:** Volume > 0 en 95%+ de los registros
4. **Outlier detection:** Identificar y manejar outliers extremos
5. **Temporal consistency:** Timestamps en orden cronol√≥gico

**C√≥digo base solicitado:**
```python
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple

class DataValidator:
    def __init__(self, tolerance_config: Dict):
        # Configurar tolerancias para validaci√≥n
        pass
    
    def validate_dataset(self, df: pd.DataFrame, symbol: str, timeframe: str) -> Dict:
        # Ejecutar todas las validaciones
        # Retornar reporte detallado
        pass
    
    def fix_common_issues(self, df: pd.DataFrame) -> pd.DataFrame:
        # Forward fill para gaps peque√±os
        # Interpolaci√≥n para outliers menores
        # Logging de modificaciones
        pass
    
    def generate_quality_report(self, validation_results: Dict) -> str:
        # Crear reporte legible para humanos
        # Incluir recomendaciones de acci√≥n
        pass
```

---

## ‚öôÔ∏è M√≥dulo 3: Feature Engineer

### Archivo: `src/data/feature_calculator.py`

**Objetivo:** Calcular todos los indicadores t√©cnicos y features para ML

**Features requeridos seg√∫n el documento de contexto:**

**Grupo 1: Price-based indicators**
- RSI (7, 14, 21 per√≠odos)
- MACD (12,26,9) + Signal + Histogram
- Bollinger Bands (20,2) + Width + Position
- SMA (5,10,20,50,100,200)
- EMA (12,26,50)

**Grupo 2: Volatility indicators**
- ATR (14)
- Bollinger Band Width
- Price deviation from moving averages

**Grupo 3: Momentum indicators**
- Rate of Change (5,10,20 per√≠odos)
- ADX (14)
- Stochastic (14,3,3)
- Williams %R (14)

**Grupo 4: Volume indicators**
- On-Balance Volume (OBV)
- Volume SMA (20)
- Volume ratio vs average
- Volume Rate of Change

**Grupo 5: Pattern features**
- Support/Resistance levels
- Trend direction (short/medium/long)
- Consolidation detection
- Breakout patterns

**C√≥digo base solicitado:**
```python
import pandas as pd
import numpy as np
import ta  # Technical Analysis library

class FeatureCalculator:
    def __init__(self):
        # Configurar par√°metros de indicadores
        pass
    
    def calculate_all_features(self, ohlcv_df: pd.DataFrame) -> pd.DataFrame:
        # Calcular todos los grupos de features
        # Manejar NaN values apropiadamente
        # Normalizar features cuando sea necesario
        pass
    
    def calculate_price_features(self, df: pd.DataFrame) -> pd.DataFrame:
        # RSI, MACD, Bollinger Bands, Moving Averages
        pass
    
    def calculate_volatility_features(self, df: pd.DataFrame) -> pd.DataFrame:
        # ATR, BB Width, Price deviations
        pass
    
    def calculate_momentum_features(self, df: pd.DataFrame) -> pd.DataFrame:
        # ROC, ADX, Stochastic, Williams %R
        pass
    
    def calculate_volume_features(self, df: pd.DataFrame) -> pd.DataFrame:
        # OBV, Volume patterns, Volume ratios
        pass
    
    def validate_features(self, features_df: pd.DataFrame) -> Dict:
        # Verificar que features est√°n en rangos esperados
        # Detectar features con demasiados NaN
        # Calcular correlaciones entre features
        pass
```

---

## üéØ M√≥dulo 4: Target Creator

### Archivo: `src/data/target_creator.py`

**Objetivo:** Crear variables target para cada modelo seg√∫n especificaciones

**Targets requeridos:**

**1. Momentum Target (‚â•5%)**
```python
def create_momentum_target(close_prices, lookforward_periods=48):
    # Buscar m√°ximo precio en pr√≥ximos 48 per√≠odos (4 horas en 5m)
    # Target = 1 si (max_future_price / current_price) >= 1.05
    # Target = 0 en caso contrario
    pass
```

**2. Rebound Target (1-3%)**
```python
def create_rebound_target(close_prices, lookforward_periods=24):
    # Buscar rebote de 1-3% en pr√≥ximos 24 per√≠odos (2 horas en 5m)
    # Target = 1 si rebote est√° en rango 1%-3%
    # Target = 0 en caso contrario
    pass
```

**3. Regime Target (Clasificaci√≥n)**
```python
def create_regime_target(close_prices, lookforward_periods=96):
    # Analizar tendencia en pr√≥ximos 96 per√≠odos (8 horas en 5m)
    # Target = 'bullish', 'bearish', 'sideways'
    # Basado en slope y volatilidad del per√≠odo
    pass
```

**C√≥digo base solicitado:**
```python
import pandas as pd
import numpy as np
from scipy import stats

class TargetCreator:
    def __init__(self, config: Dict):
        # Configurar par√°metros para cada tipo de target
        pass
    
    def create_all_targets(self, ohlcv_df: pd.DataFrame) -> pd.DataFrame:
        # Crear todos los targets y combinar
        # Manejar edge cases al final de los datos
        pass
    
    def create_momentum_targets(self, close_prices: pd.Series) -> pd.Series:
        # Implementar l√≥gica de momentum ‚â•5%
        pass
    
    def create_rebound_targets(self, close_prices: pd.Series) -> pd.Series:
        # Implementar l√≥gica de rebotes 1-3%
        pass
    
    def create_regime_targets(self, ohlcv_df: pd.DataFrame) -> pd.Series:
        # Implementar clasificaci√≥n bullish/bearish/sideways
        pass
    
    def validate_targets(self, targets_df: pd.DataFrame) -> Dict:
        # Verificar distribuci√≥n de targets
        # Asegurar que no hay data leakage
        # Calcular estad√≠sticas descriptivas
        pass
```

---

## ü§ñ M√≥dulo 5: Model Training Pipeline

### Archivo: `scripts/train_models.py`

**Objetivo:** Entrenar y validar todos los modelos con metodolog√≠a correcta

**Modelos a entrenar:**
1. **Momentum Detector:** XGBoost + RandomForest ensemble
2. **Regime Classifier:** SVM multiclass
3. **Rebound Predictor:** LSTM neural network

**Metodolog√≠a requerida:**
- **Temporal split:** 70% train, 15% validation, 15% test
- **No data leakage:** Estricto orden temporal
- **Cross-validation:** TimeSeriesSplit para hyperparameter tuning
- **Early stopping:** Para prevenir overfitting
- **Walk-forward validation:** Para validaci√≥n final

**C√≥digo base solicitado:**
```python
import pandas as pd
import numpy as np
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import accuracy_score, precision_score, recall_score
import xgboost as xgb
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVC
import tensorflow as tf
import joblib

class ModelTrainer:
    def __init__(self, config: Dict):
        # Configurar par√°metros de modelos
        # Setup de m√©tricas de evaluaci√≥n
        pass
    
    def temporal_split(self, df: pd.DataFrame) -> Tuple:
        # Implementar split temporal sin leakage
        pass
    
    def train_momentum_model(self, X_train, y_train, X_val, y_val):
        # Entrenar ensemble XGBoost + RandomForest
        # Hyperparameter tuning con TimeSeriesSplit
        # Validaci√≥n y early stopping
        pass
    
    def train_regime_model(self, X_train, y_train, X_val, y_val):
        # Entrenar SVM multiclass
        # Optimizar par√°metros C y gamma
        pass
    
    def train_rebound_model(self, X_train, y_train, X_val, y_val):
        # Construir y entrenar LSTM
        # Secuencias temporales apropiadas
        pass
    
    def evaluate_model(self, model, X_test, y_test, model_type):
        # Calcular m√©tricas espec√≠ficas de trading
        # Directional accuracy, precision/recall por clase
        # Sharpe ratio simulado
        pass
    
    def walk_forward_validation(self, df: pd.DataFrame, model, periods=6):
        # Implementar validaci√≥n walk-forward
        # Simular reentrenamiento peri√≥dico
        pass
    
    def save_models(self, models: Dict, metrics: Dict):
        # Guardar modelos entrenados
        # Guardar m√©tricas de performance
        # Crear reporte de entrenamiento
        pass
```

---

## üìã Script Principal de Entrenamiento

### Archivo: `scripts/full_training_pipeline.py`

**Objetivo:** Orchestrar todo el proceso de entrenamiento

**Flujo completo:**
1. Activar entorno virtual
2. Descargar/verificar datos hist√≥ricos
3. Validar calidad de datos
4. Calcular features
5. Crear targets
6. Entrenar modelos
7. Validar y guardar resultados

**C√≥digo base solicitado:**
```python
import os
import sys
import logging
from datetime import datetime

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def main():
    logger.info("Starting NvBot3 training pipeline...")
    
    # 1. Verificar entorno virtual
    if 'nvbot3_env' not in sys.executable:
        logger.error("Virtual environment not active!")
        sys.exit(1)
    
    # 2. Setup directorios
    ensure_directories()
    
    # 3. Descargar datos hist√≥ricos
    downloader = HistoricalDataDownloader()
    downloader.download_all_data()
    
    # 4. Validar datos
    validator = DataValidator(config)
    validation_results = validator.validate_all_datasets()
    
    # 5. Calcular features
    feature_calc = FeatureCalculator()
    features_data = feature_calc.process_all_symbols()
    
    # 6. Crear targets
    target_creator = TargetCreator(config)
    targets_data = target_creator.create_all_targets(features_data)
    
    # 7. Entrenar modelos
    trainer = ModelTrainer(config)
    models, metrics = trainer.train_all_models(features_data, targets_data)
    
    # 8. Guardar resultados
    trainer.save_models(models, metrics)
    
    logger.info("Training pipeline completed successfully!")

if __name__ == "__main__":
    main()
```

---

## ‚öôÔ∏è Configuraci√≥n y Dependencias

### Archivo: `requirements.txt` (espec√≠fico para entrenamiento)
```
numpy==1.24.3
pandas==2.0.3
scikit-learn==1.3.0
xgboost==1.7.6
tensorflow==2.13.0
ccxt==4.0.78
ta==0.10.2
joblib==1.3.1
tqdm==4.65.0
scipy==1.11.1
matplotlib==3.7.2
seaborn==0.12.2
python-dotenv==1.0.0
pyyaml==6.0.1
```

### Archivo: `config/training_config.yaml`
```yaml
data:
  symbols: ['BTCUSDT', 'ETHUSDT', 'BNBUSDT', 'ADAUSDT', 'SOLUSDT']
  timeframes: ['5m', '15m', '1h', '4h', '1d']
  start_date: '2022-01-01'
  lookback_periods: 200

models:
  momentum:
    xgb_params:
      n_estimators: 200
      max_depth: 8
      learning_rate: 0.1
      subsample: 0.8
    rf_params:
      n_estimators: 100
      max_depth: 10
      min_samples_split: 5

  regime:
    svm_params:
      C: 1.0
      kernel: 'rbf'
      gamma: 'scale'

  rebound:
    lstm_params:
      sequence_length: 24
      lstm_units: 50
      dropout: 0.2
      epochs: 100

validation:
  train_ratio: 0.7
  val_ratio: 0.15
  test_ratio: 0.15
  cv_folds: 5
  walk_forward_periods: 6
```

---

## üéØ Comandos de Ejecuci√≥n

### Para GitHub Copilot: Secuencia de comandos a usar

```bash
# 1. Activar entorno (SIEMPRE PRIMERO)
nvbot3_env\Scripts\activate  # Windows
# source nvbot3_env/bin/activate  # Linux/Mac

# 2. Instalar dependencias si es primera vez
pip install -r requirements.txt

# 3. Ejecutar pipeline completo de entrenamiento
python scripts/full_training_pipeline.py

# 4. O ejecutar m√≥dulos individuales para testing
python scripts/download_historical_data.py
python -m src.data.feature_calculator --symbol BTCUSDT --timeframe 5m
python scripts/train_models.py --model momentum

# 5. Validar resultados
python scripts/validate_trained_models.py
```

---

## ‚ö†Ô∏è Instrucciones Cr√≠ticas para Copilot

1. **SIEMPRE verificar que el entorno virtual est√° activo antes de cualquier operaci√≥n**
2. **Usar temporal splits, NUNCA random splits para datos financieros**
3. **Implementar progress bars para operaciones largas (tqdm)**
4. **Incluir extensive error handling y logging**
5. **Validar datos en cada paso antes de continuar**
6. **Guardar checkpoints para poder resumir procesos largos**
7. **Usar configuraci√≥n YAML para parametrizaci√≥n, no hardcoding**
8. **Implementar memory-efficient processing para datasets grandes**

---

*Estas instrucciones deben seguirse estrictamente para asegurar un entrenamiento robusto y calibrado del NvBot3.*
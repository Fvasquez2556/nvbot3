# 游댢 CORRECCI칍N ESPEC칈FICA - regularized_models.py
# M칠todo fit() en la clase RegularizedXGBoost l칤nea 70-120

def fit(self, X_train: pd.DataFrame, y_train: pd.Series, 
        X_val: Optional[pd.DataFrame] = None, y_val: Optional[pd.Series] = None,
        eval_set: Optional[list] = None, **kwargs):
    """
    CORRECCI칍N: Entrenar modelo que acepta eval_set como par치metro.
    """
    # Convertir a array si es DataFrame/Series para consistencia
    if hasattr(X_train, 'values'):
        X_train_array = X_train.values
    else:
        X_train_array = np.array(X_train)
        
    if hasattr(y_train, 'values'):
        y_train_array = y_train.values
    else:
        y_train_array = np.array(y_train)

    # Verificar que tenemos features
    if X_train_array.shape[1] == 0:
        raise ValueError(f"X_train no tiene features! Shape: {X_train_array.shape}")
    
    # Normalizar features
    X_train_scaled = self.scaler.fit_transform(X_train_array)
    
    # Selecci칩n de features para reducir dimensionalidad
    X_train_selected = self.feature_selector.fit_transform(X_train_scaled, y_train_array)
    
    # Preparar eval_set - priorizar par치metro eval_set sobre X_val/y_val
    eval_set_processed = None
    X_val_selected = None
    
    if eval_set is not None and len(eval_set) > 0:
        try:
            X_val_raw, y_val_raw = eval_set[0]
            X_val_array = X_val_raw.values if hasattr(X_val_raw, 'values') else np.array(X_val_raw)
            y_val_array = y_val_raw.values if hasattr(y_val_raw, 'values') else np.array(y_val_raw)
            
            X_val_scaled = self.scaler.transform(X_val_array)
            X_val_selected = self.feature_selector.transform(X_val_scaled)
            eval_set_processed = [(X_val_selected, y_val_array)]
        except Exception as e:
            self.logger.warning(f"Error procesando eval_set: {e}")
    elif X_val is not None and y_val is not None:
        X_val_array = X_val.values if hasattr(X_val, 'values') else np.array(X_val)
        y_val_array = y_val.values if hasattr(y_val, 'values') else np.array(y_val)
        
        X_val_scaled = self.scaler.transform(X_val_array)
        X_val_selected = self.feature_selector.transform(X_val_scaled)
        eval_set_processed = [(X_val_selected, y_val_array)]
    
    # CORRECCI칍N CR칈TICA: Filtrar par치metros problem치ticos antes de crear modelo
    safe_params = self.params.copy()
    
    # Algunos par치metros pueden causar problemas seg칰n la versi칩n de XGBoost
    problematic_params = ['objective']  # Puede ser rechazado en algunos contextos
    
    # Crear modelo XGBoost con par치metros seguros
    try:
        self.model = xgb.XGBRegressor(**safe_params)
    except Exception as e:
        self.logger.warning(f"Error con par치metros completos: {e}")
        # Fallback: usar par치metros m칤nimos
        minimal_params = {
            'n_estimators': safe_params.get('n_estimators', 100),
            'max_depth': safe_params.get('max_depth', 4),
            'learning_rate': safe_params.get('learning_rate', 0.05),
            'random_state': safe_params.get('random_state', 42)
        }
        self.model = xgb.XGBRegressor(**minimal_params)
        self.logger.info("Usando par치metros m칤nimos para XGBoost")
    
    # Early stopping solo si hay datos de validaci칩n
    fit_params = {'verbose': False}
    
    if eval_set_processed:
        fit_params['eval_set'] = eval_set_processed
        fit_params['early_stopping_rounds'] = 15
    
    # Entrenar modelo
    try:
        self.model.fit(X_train_selected, y_train_array, **fit_params)
    except Exception as e:
        self.logger.warning(f"Error en fit con eval_set: {e}")
        # Fallback: entrenar sin eval_set
        self.model.fit(X_train_selected, y_train_array, verbose=False)
    
    # Log de resultados
    selected_features = self.feature_selector.get_support()
    
    self.logger.info(f"Modelo {self.task_type} entrenado:")
    self.logger.info(f"  Features seleccionadas: {np.sum(selected_features)}/{len(selected_features)}")
    self.logger.info(f"  Early stopping en iteraci칩n: {getattr(self.model, 'best_iteration', 'N/A')}")
    
    try:
        train_score = self.model.score(X_train_selected, y_train_array)
        self.logger.info(f"  Score en training: {train_score:.4f}")
        
        if eval_set_processed and X_val_selected is not None:
            val_score = self.model.score(X_val_selected, y_val_array)
            overfitting_gap = train_score - val_score
            
            self.logger.info(f"  Score en validaci칩n: {val_score:.4f}")
            self.logger.info(f"  Gap train-val: {overfitting_gap:.4f}")
            
            if overfitting_gap > 0.15:
                self.logger.warning(f"丘멆잺  OVERFITTING DETECTADO: Gap {overfitting_gap:.4f} > 0.15!")
    except Exception as e:
        self.logger.warning(f"Error calculando scores: {e}")

def predict(self, X: pd.DataFrame) -> np.ndarray:
    """Hacer predicciones con el modelo entrenado."""
    if self.model is None:
        raise ValueError("Modelo no ha sido entrenado")
    
    # Convertir a array si es necesario
    if hasattr(X, 'values'):
        X_array = X.values
    else:
        X_array = np.array(X)
    
    # Verificar que tenemos features
    if X_array.shape[1] == 0:
        raise ValueError(f"X no tiene features para predicci칩n! Shape: {X_array.shape}")
    
    X_scaled = self.scaler.transform(X_array)
    X_selected = self.feature_selector.transform(X_scaled)
    return self.model.predict(X_selected)
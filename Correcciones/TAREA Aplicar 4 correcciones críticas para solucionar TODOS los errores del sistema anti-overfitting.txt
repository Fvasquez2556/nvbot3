üìù INSTRUCCIONES ESPEC√çFICAS PARA GITHUB COPILOT AGENT
üéØ TAREA: Aplicar 4 correcciones cr√≠ticas para solucionar TODOS los errores del sistema anti-overfitting

üìÇ CORRECCI√ìN 1: src/validation/walk_forward_validator.py
üî¥ PROBLEMA: Found array with 0 feature(s) - Features mal extra√≠das
‚úÖ ACCI√ìN: En el m√©todo _train_and_evaluate_period() l√≠nea ~156-160, REEMPLAZAR:
python# BUSCAR ESTA L√çNEA:
feature_cols = [col for col in train_data.columns if col.startswith('feature_')]

# REEMPLAZAR POR:
exclude_cols = {'target', 'timestamp', 'close_time', 'date'}
feature_cols = [col for col in train_data.columns if col not in exclude_cols]

# Verificaci√≥n de seguridad
if not feature_cols:
    known_features = ['price', 'volume', 'rsi', 'ma_20', 'ma_50', 'volatility_20', 'returns']
    feature_cols = [col for col in known_features if col in train_data.columns]
    
    if not feature_cols:
        raise ValueError(f"No se encontraron features v√°lidas. Columnas: {list(train_data.columns)}")

# Log para debugging
self.logger.info(f"Features detectadas: {feature_cols}")

# Verificar que target existe
target_col = 'target'
if target_col not in train_data.columns:
    raise ValueError(f"Columna target '{target_col}' no encontrada")

X_train = train_data[feature_cols]
y_train = train_data[target_col]
X_test = test_data[feature_cols]
y_test = test_data[target_col]

# Verificaci√≥n final
assert X_train.shape[1] > 0, f"X_train tiene 0 features! Features: {feature_cols}"
assert X_test.shape[1] > 0, f"X_test tiene 0 features! Features: {feature_cols}"
‚úÖ ACCI√ìN 2: En la misma funci√≥n, cambiar el entrenamiento del modelo:
python# BUSCAR:
model.fit(X_train_fold, y_train_fold, eval_set=...)

# REEMPLAZAR POR:
model.fit(X_train, y_train)  # Solo argumentos b√°sicos

üìÇ CORRECCI√ìN 2: src/models/regularized_models.py
üî¥ PROBLEMA: RegularizedXGBoost.__init__() got an unexpected keyword argument 'objective'
‚úÖ ACCI√ìN: En el m√©todo fit() de la clase RegularizedXGBoost, AGREGAR despu√©s de la l√≠nea donde se prepara eval_set:
python# AGREGAR ESTA SECCI√ìN ANTES DE CREAR EL MODELO:

# CORRECCI√ìN CR√çTICA: Filtrar par√°metros problem√°ticos
safe_params = self.params.copy()

# Crear modelo XGBoost con manejo de errores
try:
    self.model = xgb.XGBRegressor(**safe_params)
except Exception as e:
    self.logger.warning(f"Error con par√°metros completos: {e}")
    # Fallback: usar par√°metros m√≠nimos
    minimal_params = {
        'n_estimators': safe_params.get('n_estimators', 100),
        'max_depth': safe_params.get('max_depth', 4),
        'learning_rate': safe_params.get('learning_rate', 0.05),
        'random_state': safe_params.get('random_state', 42)
    }
    self.model = xgb.XGBRegressor(**minimal_params)
    self.logger.info("Usando par√°metros m√≠nimos para XGBoost")
‚úÖ ACCI√ìN 2: En el mismo m√©todo, REEMPLAZAR el fit del modelo:
python# BUSCAR:
self.model.fit(X_train_selected, y_train_array, **fit_params)

# REEMPLAZAR POR:
try:
    self.model.fit(X_train_selected, y_train_array, **fit_params)
except Exception as e:
    self.logger.warning(f"Error en fit con eval_set: {e}")
    # Fallback: entrenar sin eval_set
    self.model.fit(X_train_selected, y_train_array, verbose=False)
‚úÖ ACCI√ìN 3: En el m√©todo predict(), AGREGAR verificaci√≥n:
python# AGREGAR AL INICIO DEL M√âTODO predict():
if hasattr(X, 'values'):
    X_array = X.values
else:
    X_array = np.array(X)

# Verificar que tenemos features
if X_array.shape[1] == 0:
    raise ValueError(f"X no tiene features para predicci√≥n! Shape: {X_array.shape}")

üìÇ CORRECCI√ìN 3: src/validation/overfitting_detector.py
üî¥ PROBLEMA: 'numpy.ndarray' object has no attribute 'columns'
‚úÖ ACCI√ìN: En el m√©todo detect(), REEMPLAZAR las primeras l√≠neas despu√©s de try::
python# REEMPLAZAR EL INICIO DEL M√âTODO detect() POR:

# CORRECCI√ìN CR√çTICA: Convertir a tipos consistentes
def safe_convert_to_array(data):
    """Convertir datos a numpy array de forma segura."""
    if hasattr(data, 'values'):
        return data.values
    elif isinstance(data, (list, tuple)):
        return np.array(data)
    else:
        return np.array(data)

# Convertir todos los datos a arrays numpy
X_train_array = safe_convert_to_array(X_train)
y_train_array = safe_convert_to_array(y_train)
X_val_array = safe_convert_to_array(X_val)
y_val_array = safe_convert_to_array(y_val)

# Verificar que tenemos datos v√°lidos
if X_train_array.shape[0] == 0 or X_val_array.shape[0] == 0:
    raise ValueError("Datos vac√≠os para an√°lisis de overfitting")

if X_train_array.shape[1] == 0:
    raise ValueError("No hay features para an√°lisis de overfitting")

# 1. Scores b√°sicos
train_score = model.score(X_train_array, y_train_array)
val_score = model.score(X_val_array, y_val_array)
gap = train_score - val_score
‚úÖ ACCI√ìN 2: AGREGAR manejo de errores para predicciones:
python# REEMPLAZAR LA SECCI√ìN DE PREDICCIONES POR:
try:
    y_train_pred = model.predict(X_train_array)
    y_val_pred = model.predict(X_val_array)
except Exception as e:
    self.logger.warning(f"Error en predicciones: {e}")
    # Usar scores como fallback
    y_train_pred = np.full(len(y_train_array), np.mean(y_train_array))
    y_val_pred = np.full(len(y_val_array), np.mean(y_val_array))

üìÇ CORRECCI√ìN 4: scripts/demo_anti_overfitting.py
üî¥ PROBLEMA: Datos insuficientes y manejo inconsistente
‚úÖ ACCI√ìN 1: CAMBIAR la funci√≥n create_sample_crypto_data():
python# BUSCAR:
def create_sample_crypto_data(n_samples: int = 3000) -> pd.DataFrame:

# REEMPLAZAR POR:
def create_sample_crypto_data(n_samples: int = 5000) -> pd.DataFrame:  # M√°s datos
‚úÖ ACCI√ìN 2: AGREGAR al final de create_sample_crypto_data():
python# AGREGAR ANTES DEL return df:
# CORRECCI√ìN CR√çTICA: Asegurar nombres consistentes
logger.info(f"Datos creados con columnas: {list(df.columns)}")
logger.info(f"Shape de datos: {df.shape}")
‚úÖ ACCI√ìN 3: En todas las funciones de demo, REEMPLAZAR las l√≠neas de entrenamiento:
python# BUSCAR L√çNEAS COMO:
model.fit(X_train, y_train, X_val, y_val)
model_ensemble.fit(X_train, y_train, X_val, y_val)

# REEMPLAZAR POR:
model.fit(X_train, y_train)  # Solo argumentos b√°sicos
model_ensemble.fit(X_train, y_train)  # Solo argumentos b√°sicos
‚úÖ ACCI√ìN 4: AGREGAR verificaciones en demo_overfitting_detection():
python# AGREGAR DESPU√âS DE DEFINIR feature_cols:
# Verificar columnas
if not all(col in df.columns for col in feature_cols + [target_col]):
    logger.error(f"Columnas faltantes. Disponibles: {list(df.columns)}")
    return

logger.info(f"Train shape: {X_train.shape}, Val shape: {X_val.shape}")

üöÄ COMANDOS DE VERIFICACI√ìN
Despu√©s de aplicar TODAS las correcciones:
bash# 1. Activar entorno
nvbot3_env\Scripts\activate

# 2. Ejecutar demo corregido
python scripts/demo_anti_overfitting.py

# 3. Verificar que NO aparezcan estos errores:
# ‚ùå "Found array with 0 feature(s)"
# ‚ùå "RegularizedXGBoost.__init__() got an unexpected keyword argument"
# ‚ùå "'numpy.ndarray' object has no attribute 'columns'"

‚úÖ RESULTADO ESPERADO
Despu√©s de estas correcciones:

‚úÖ Walk-forward validation funcionar√° - Con m√∫ltiples iteraciones exitosas
‚úÖ RegularizedXGBoost se entrenar√° sin errores - Manejo robusto de par√°metros
‚úÖ Overfitting detection trabajar√° con arrays - Sin errores de .columns
‚úÖ Demo completar√° sin crashes - Con datos consistentes
#!/usr/bin/env python3
"""
üìä CSV Format Validator - NvBot3
===============================

Validador completo de formato CSV para garantizar compatibilidad con:
- Feature Calculator (indicadores t√©cnicos)
- Target Creator (detecci√≥n momentum/rebound)
- Model Trainer (pipeline de entrenamiento)

Basado en el formato usado por download_training_data_only.py
Detecta problemas cr√≠ticos vs warnings para optimizar calidad de datos.
"""

import os
import sys
import pandas as pd
import numpy as np
import json
import logging
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional
import warnings

# Configurar logging
log_file = Path(__file__).parent.parent / 'logs' / 'csv_validation_report.log'
log_file.parent.mkdir(exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler(log_file, encoding='utf-8')
    ]
)

logger = logging.getLogger(__name__)
warnings.filterwarnings('ignore')

class CSVFormatValidator:
    """
    Validador de formato CSV espec√≠fico para NvBot3
    Garantiza compatibilidad con Feature Calculator y Target Creator
    """
    
    def __init__(self):
        # Directorio de datos raw
        self.raw_data_dir = Path(__file__).parent.parent / 'data' / 'raw'
        
        # Formato est√°ndar requerido (compatible con HistoricalDataDownloader)
        self.required_columns = ['timestamp', 'open', 'high', 'low', 'close', 'volume']
        self.optional_columns = ['close_time', 'quote_volume', 'trades', 'taker_buy_base', 'taker_buy_quote']
        
        # Criterios de calidad espec√≠ficos para entrenamiento
        self.quality_criteria = {
            'min_records_for_features': 200,      # Para SMA 200
            'min_records_for_training': 1000,     # Para training split confiable
            'max_null_percentage': 5.0,           # Max 5% valores nulos
            'max_extreme_change_percentage': 1.0, # Max 1% cambios extremos (>500%)
            'max_gap_percentage': 2.0,            # Max 2% gaps temporales
            'lookforward_periods': {              # Para Target Creator
                '5m': 48,   # 4h ahead
                '15m': 16,  # 4h ahead  
                '1h': 4,    # 4h ahead
                '4h': 1,    # 4h ahead
                '1d': 1     # 1d ahead
            }
        }
        
        # Estad√≠sticas globales
        self.total_files = 0
        self.valid_files = 0
        self.files_with_warnings = 0
        self.files_with_errors = 0
        self.validation_results = {}
        
    def validate_all_csv_files(self) -> bool:
        """
        Validar todos los archivos CSV en data/raw/
        Retorna True si todos son v√°lidos para entrenamiento
        """
        logger.info("üîç INICIANDO VALIDACI√ìN COMPLETA DE ARCHIVOS CSV")
        logger.info("=" * 55)
        logger.info(f"üìÅ Directorio: {self.raw_data_dir}")
        
        if not self.raw_data_dir.exists():
            logger.error(f"‚ùå Directorio de datos no existe: {self.raw_data_dir}")
            return False
        
        # Obtener todos los archivos CSV
        csv_files = list(self.raw_data_dir.glob("*.csv"))
        
        if not csv_files:
            logger.warning("‚ö†Ô∏è No se encontraron archivos CSV en el directorio")
            return False
        
        self.total_files = len(csv_files)
        logger.info(f"üìä Total archivos CSV encontrados: {self.total_files}")
        logger.info("")
        
        # Validar cada archivo
        for i, csv_file in enumerate(csv_files, 1):
            logger.info(f"üîç [{i}/{self.total_files}] Validando: {csv_file.name}")
            
            validation_result = self._validate_single_file(csv_file)
            self.validation_results[csv_file.name] = validation_result
            
            if validation_result['is_valid']:
                self.valid_files += 1
                if validation_result['warnings']:
                    self.files_with_warnings += 1
                    logger.warning(f"‚ö†Ô∏è [{i}/{self.total_files}] {csv_file.name}: V√°lido con advertencias")
                else:
                    logger.info(f"‚úÖ [{i}/{self.total_files}] {csv_file.name}: V√°lido")
            else:
                self.files_with_errors += 1
                logger.error(f"‚ùå [{i}/{self.total_files}] {csv_file.name}: ERRORES CR√çTICOS")
                
                # Mostrar errores cr√≠ticos
                for error in validation_result['critical_errors']:
                    logger.error(f"   ‚Ä¢ {error}")
        
        # Generar reporte final
        self._generate_final_report()
        
        # Retornar si todos los archivos est√°n listos para entrenamiento
        return self.files_with_errors == 0
    
    def _validate_single_file(self, csv_file: Path) -> Dict:
        """
        Validar un archivo CSV individual
        Retorna diccionario con resultados de validaci√≥n
        """
        result = {
            'filename': csv_file.name,
            'is_valid': True,
            'critical_errors': [],
            'warnings': [],
            'stats': {},
            'feature_calculator_ready': False,
            'target_creator_ready': False,
            'training_ready': False
        }
        
        try:
            # Cargar CSV
            df = pd.read_csv(csv_file)
            result['stats']['total_records'] = len(df)
            
            if len(df) == 0:
                result['critical_errors'].append("empty_file: Archivo vac√≠o")
                result['is_valid'] = False
                return result
            
            # 1. Validar estructura de columnas
            self._validate_columns(df, result)
            
            # 2. Validar tipos de datos
            self._validate_data_types(df, result)
            
            # 3. Validar timestamps
            self._validate_timestamps(df, result)
            
            # 4. Validar datos OHLCV
            self._validate_ohlcv_data(df, result)
            
            # 5. Validar suficiencia de datos
            self._validate_data_sufficiency(df, csv_file.name, result)
            
            # 6. Validar continuidad temporal
            self._validate_temporal_continuity(df, csv_file.name, result)
            
            # 7. Validar compatibilidad con componentes
            self._validate_component_compatibility(df, csv_file.name, result)
            
        except Exception as e:
            result['critical_errors'].append(f"file_read_error: {str(e)}")
            result['is_valid'] = False
        
        # Determinar si hay errores cr√≠ticos
        if result['critical_errors']:
            result['is_valid'] = False
        
        return result
    
    def _validate_columns(self, df: pd.DataFrame, result: Dict):
        """Validar que existan las columnas requeridas"""
        missing_columns = [col for col in self.required_columns if col not in df.columns]
        
        if missing_columns:
            result['critical_errors'].append(f"missing_columns: Columnas faltantes: {missing_columns}")
        
        # Verificar columnas extra (no cr√≠tico)
        extra_columns = [col for col in df.columns if col not in self.required_columns + self.optional_columns]
        if extra_columns:
            result['warnings'].append(f"extra_columns: Columnas adicionales: {extra_columns}")
        
        result['stats']['columns'] = list(df.columns)
    
    def _validate_data_types(self, df: pd.DataFrame, result: Dict):
        """Validar tipos de datos de columnas cr√≠ticas"""
        numeric_columns = ['open', 'high', 'low', 'close', 'volume']
        
        for col in numeric_columns:
            if col in df.columns:
                if not pd.api.types.is_numeric_dtype(df[col]):
                    result['critical_errors'].append(f"invalid_data_type: {col} no es num√©rico")
                
                # Verificar valores nulos
                null_count = df[col].isnull().sum()
                null_percentage = (null_count / len(df)) * 100
                
                if null_percentage > self.quality_criteria['max_null_percentage']:
                    result['critical_errors'].append(f"excessive_nulls: {col} tiene {null_percentage:.1f}% valores nulos")
                elif null_count > 0:
                    result['warnings'].append(f"some_nulls: {col} tiene {null_count} valores nulos ({null_percentage:.1f}%)")
    
    def _validate_timestamps(self, df: pd.DataFrame, result: Dict):
        """Validar formato y orden de timestamps"""
        if 'timestamp' not in df.columns:
            return
        
        try:
            # Intentar convertir timestamps
            timestamps = pd.to_datetime(df['timestamp'])
            
            # Verificar orden cronol√≥gico
            if not timestamps.is_monotonic_increasing:
                result['critical_errors'].append("temporal_order: Timestamps no est√°n ordenados cronol√≥gicamente")
            
            # Verificar duplicados
            duplicates = timestamps.duplicated().sum()
            if duplicates > 0:
                result['warnings'].append(f"duplicate_timestamps: {duplicates} timestamps duplicados")
            
            result['stats']['date_range'] = {
                'start': str(timestamps.min()),
                'end': str(timestamps.max()),
                'duration_days': (timestamps.max() - timestamps.min()).days
            }
            
        except Exception as e:
            result['critical_errors'].append(f"timestamp_format: Error parseando timestamps: {str(e)}")
    
    def _validate_ohlcv_data(self, df: pd.DataFrame, result: Dict):
        """Validar consistencia de datos OHLCV"""
        required_ohlc = ['open', 'high', 'low', 'close']
        
        if not all(col in df.columns for col in required_ohlc):
            return
        
        # Verificar relaciones OHLC v√°lidas
        invalid_ohlc = (
            (df['high'] < df['low']) | 
            (df['high'] < df['open']) | 
            (df['high'] < df['close']) |
            (df['low'] > df['open']) | 
            (df['low'] > df['close'])
        )
        
        invalid_count = invalid_ohlc.sum()
        invalid_percentage = (invalid_count / len(df)) * 100
        
        if invalid_percentage > 1.0:  # M√°s del 1% inv√°lido es cr√≠tico
            result['critical_errors'].append(f"invalid_ohlc_relationships: {invalid_count} registros con relaciones OHLC inv√°lidas ({invalid_percentage:.1f}%)")
        elif invalid_count > 0:
            result['warnings'].append(f"some_invalid_ohlc: {invalid_count} registros con relaciones OHLC inv√°lidas ({invalid_percentage:.1f}%)")
        
        # Verificar cambios extremos de precio
        if 'close' in df.columns:
            price_changes = df['close'].pct_change().abs()
            extreme_changes = (price_changes > 5.0).sum()  # >500% cambio
            extreme_percentage = (extreme_changes / len(df)) * 100
            
            if extreme_percentage > self.quality_criteria['max_extreme_change_percentage']:
                result['critical_errors'].append(f"excessive_extreme_changes: {extreme_changes} cambios extremos de precio (>{extreme_percentage:.1f}%)")
            elif extreme_changes > 0:
                result['warnings'].append(f"some_extreme_changes: {extreme_changes} cambios extremos de precio ({extreme_percentage:.1f}%)")
        
        # Verificar volumen
        if 'volume' in df.columns:
            zero_volume = (df['volume'] <= 0).sum()
            if zero_volume > len(df) * 0.1:  # >10% volumen cero es preocupante
                result['warnings'].append(f"high_zero_volume: {zero_volume} registros con volumen cero o negativo")
    
    def _validate_data_sufficiency(self, df: pd.DataFrame, filename: str, result: Dict):
        """Validar suficiencia de datos para Feature Calculator y Target Creator"""
        record_count = len(df)
        
        # Para Feature Calculator (necesita indicadores largos)
        if record_count < self.quality_criteria['min_records_for_features']:
            result['critical_errors'].append(f"insufficient_data_for_features: {record_count} < {self.quality_criteria['min_records_for_features']} per√≠odos")
        else:
            result['feature_calculator_ready'] = True
        
        # Para entrenamiento confiable
        if record_count < self.quality_criteria['min_records_for_training']:
            result['warnings'].append(f"limited_training_data: {record_count} < {self.quality_criteria['min_records_for_training']} per√≠odos recomendados")
        
        # Para Target Creator (necesita per√≠odos futuros)
        timeframe = self._extract_timeframe_from_filename(filename)
        if timeframe and timeframe in self.quality_criteria['lookforward_periods']:
            required_lookforward = self.quality_criteria['lookforward_periods'][timeframe]
            usable_records = record_count - required_lookforward
            
            if usable_records < self.quality_criteria['min_records_for_features']:
                result['critical_errors'].append(f"insufficient_usable_data: Solo {usable_records} registros utilizables despu√©s de lookforward")
            else:
                result['target_creator_ready'] = True
    
    def _validate_temporal_continuity(self, df: pd.DataFrame, filename: str, result: Dict):
        """Validar continuidad temporal apropiada para el timeframe"""
        if 'timestamp' not in df.columns:
            return
        
        try:
            timestamps = pd.to_datetime(df['timestamp'])
            
            # Determinar timeframe esperado
            timeframe = self._extract_timeframe_from_filename(filename)
            if timeframe:
                expected_delta = self._get_expected_timedelta(timeframe)
            else:
                expected_delta = None
            
            if expected_delta:
                # Calcular diferencias temporales
                time_diffs = timestamps.diff().dropna()
                
                # Detectar gaps grandes (>2x el intervalo esperado)
                large_gaps = (time_diffs > expected_delta * 2).sum()
                gap_percentage = (large_gaps / len(time_diffs)) * 100
                
                if gap_percentage > self.quality_criteria['max_gap_percentage']:
                    result['critical_errors'].append(f"excessive_temporal_gaps: {large_gaps} gaps grandes ({gap_percentage:.1f}%)")
                elif large_gaps > 0:
                    result['warnings'].append(f"temporal_gaps: {large_gaps} gaps grandes detectados ({gap_percentage:.1f}%)")
                
                result['stats']['temporal_analysis'] = {
                    'expected_interval': str(expected_delta),
                    'large_gaps': int(large_gaps),
                    'gap_percentage': round(gap_percentage, 2)
                }
                
        except Exception as e:
            result['warnings'].append(f"temporal_analysis_failed: {str(e)}")
    
    def _validate_component_compatibility(self, df: pd.DataFrame, filename: str, result: Dict):
        """Validar compatibilidad espec√≠fica con Feature Calculator y Target Creator"""
        
        # Verificar que est√© listo para Feature Calculator
        if result['feature_calculator_ready'] and not result['critical_errors']:
            # Verificar datos para indicadores t√©cnicos comunes
            if 'close' in df.columns and len(df) >= 200:
                result['feature_calculator_ready'] = True
            else:
                result['feature_calculator_ready'] = False
        
        # Verificar que est√© listo para Target Creator
        if result['target_creator_ready'] and not result['critical_errors']:
            timeframe = self._extract_timeframe_from_filename(filename)
            if timeframe and 'close' in df.columns:
                lookforward = self.quality_criteria['lookforward_periods'].get(timeframe, 1)
                if len(df) > lookforward + 200:
                    result['target_creator_ready'] = True
                else:
                    result['target_creator_ready'] = False
        
        # Archivo listo para entrenamiento si ambos componentes est√°n listos
        result['training_ready'] = (result['feature_calculator_ready'] and 
                                   result['target_creator_ready'] and 
                                   not result['critical_errors'])
    
    def _extract_timeframe_from_filename(self, filename: str) -> Optional[str]:
        """Extraer timeframe del nombre del archivo (ej: BTCUSDT_5m.csv -> 5m)"""
        try:
            # Formato esperado: SYMBOL_TIMEFRAME.csv
            parts = filename.replace('.csv', '').split('_')
            if len(parts) >= 2:
                return parts[-1]  # √öltimo elemento es el timeframe
        except:
            pass
        return None
    
    def _get_expected_timedelta(self, timeframe: str) -> Optional[timedelta]:
        """Obtener timedelta esperado para un timeframe"""
        timeframe_map = {
            '1m': timedelta(minutes=1),
            '5m': timedelta(minutes=5),
            '15m': timedelta(minutes=15),
            '1h': timedelta(hours=1),
            '4h': timedelta(hours=4),
            '1d': timedelta(days=1)
        }
        return timeframe_map.get(timeframe)
    
    def _generate_final_report(self):
        """Generar reporte final de validaci√≥n"""
        logger.info("")
        logger.info("=" * 55)
        logger.info("üìä REPORTE FINAL DE VALIDACI√ìN CSV")
        logger.info("=" * 55)
        
        # Estad√≠sticas generales
        success_rate = (self.valid_files / self.total_files * 100) if self.total_files > 0 else 0
        
        logger.info(f"üìÅ Total archivos analizados: {self.total_files}")
        logger.info(f"‚úÖ Archivos v√°lidos: {self.valid_files}")
        logger.info(f"‚ö†Ô∏è Archivos con problemas: {self.files_with_warnings}")
        logger.info(f"üö® Errores cr√≠ticos: {self.files_with_errors}")
        logger.info(f"üìà Tasa de √©xito: {success_rate:.1f}%")
        logger.info("")
        
        # Compatibilidad por componente
        feature_ready = sum(1 for r in self.validation_results.values() if r.get('feature_calculator_ready', False))
        target_ready = sum(1 for r in self.validation_results.values() if r.get('target_creator_ready', False))
        training_ready = sum(1 for r in self.validation_results.values() if r.get('training_ready', False))
        
        logger.info("ü§ñ COMPATIBILIDAD PARA ENTRENAMIENTO:")
        logger.info(f"üîß Feature Calculator listos: {feature_ready}/{self.total_files}")
        logger.info(f"üéØ Target Creator listos: {target_ready}/{self.total_files}")
        logger.info(f"üöÄ Listos para entrenamiento: {training_ready}/{self.total_files}")
        logger.info("")
        
        # Problemas m√°s comunes
        self._report_common_issues()
        
        # Recomendaciones finales
        self._provide_recommendations(training_ready)
        
        # Guardar reporte detallado en JSON
        self._save_detailed_report()
    
    def _report_common_issues(self):
        """Reportar los problemas m√°s comunes encontrados"""
        error_counts = {}
        warning_counts = {}
        
        for result in self.validation_results.values():
            for error in result.get('critical_errors', []):
                error_type = error.split(':')[0]
                error_counts[error_type] = error_counts.get(error_type, 0) + 1
            
            for warning in result.get('warnings', []):
                warning_type = warning.split(':')[0]
                warning_counts[warning_type] = warning_counts.get(warning_type, 0) + 1
        
        if error_counts:
            logger.warning("üö® ERRORES CR√çTICOS M√ÅS COMUNES:")
            for error_type, count in sorted(error_counts.items(), key=lambda x: x[1], reverse=True):
                logger.warning(f"   ‚Ä¢ {error_type}: {count} archivos")
        
        if warning_counts:
            logger.info("‚ö†Ô∏è ADVERTENCIAS M√ÅS COMUNES:")
            for warning_type, count in sorted(warning_counts.items(), key=lambda x: x[1], reverse=True)[:5]:
                logger.info(f"   ‚Ä¢ {warning_type}: {count} archivos")
    
    def _provide_recommendations(self, training_ready: int):
        """Proporcionar recomendaciones basadas en resultados"""
        logger.info("üí° RECOMENDACIONES:")
        
        if training_ready == self.total_files:
            logger.info("üéâ ¬°TODOS LOS ARCHIVOS EST√ÅN LISTOS PARA ENTRENAMIENTO!")
            logger.info("   ‚úÖ Puedes proceder con Feature Calculator")
            logger.info("   ‚úÖ Puedes proceder con Target Creator")
            logger.info("   ‚úÖ Puedes proceder con Model Trainer")
        elif training_ready >= self.total_files * 0.9:
            logger.info("üü¢ La mayor√≠a de archivos est√°n listos")
            logger.info("   üîß Corregir archivos con errores cr√≠ticos")
            logger.info("   ‚úÖ Sistema mayormente funcional")
        elif training_ready >= self.total_files * 0.7:
            logger.warning("üü° Sistema parcialmente listo")
            logger.warning("   ‚ö†Ô∏è Revisar y corregir archivos problem√°ticos")
            logger.warning("   üîÑ Re-descargar archivos con errores cr√≠ticos")
        else:
            logger.error("üî¥ SISTEMA NO LISTO PARA ENTRENAMIENTO")
            logger.error("   ‚ùå Demasiados archivos con problemas cr√≠ticos")
            logger.error("   üîÑ Re-ejecutar download_training_data_only.py")
            logger.error("   üîç Verificar configuraci√≥n de descarga")
    
    def _save_detailed_report(self):
        """Guardar reporte detallado en formato JSON"""
        report_path = Path(__file__).parent.parent / 'logs' / 'csv_validation_detailed_report.json'
        
        detailed_report = {
            'validation_timestamp': datetime.now().isoformat(),
            'summary': {
                'total_files': self.total_files,
                'valid_files': self.valid_files,
                'files_with_warnings': self.files_with_warnings,
                'files_with_errors': self.files_with_errors,
                'success_rate': (self.valid_files / self.total_files * 100) if self.total_files > 0 else 0
            },
            'quality_criteria': self.quality_criteria,
            'file_results': self.validation_results
        }
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(detailed_report, f, indent=2, ensure_ascii=False)
        
        logger.info(f"üìÑ Reporte detallado guardado: {report_path}")


def main():
    """Funci√≥n principal"""
    logger.info("üöÄ VALIDADOR CSV NVBOT3 - INICIO")
    logger.info("üéØ Objetivo: Verificar compatibilidad para entrenamiento")
    logger.info("")
    
    validator = CSVFormatValidator()
    
    try:
        success = validator.validate_all_csv_files()
        
        if success:
            logger.info("üéâ VALIDACI√ìN EXITOSA: Todos los archivos listos para entrenamiento")
            return 0
        else:
            logger.error("‚ùå VALIDACI√ìN FALLIDA: Corregir errores antes del entrenamiento")
            return 1
            
    except KeyboardInterrupt:
        logger.warning("‚ö†Ô∏è Validaci√≥n interrumpida por usuario")
        return 1
    except Exception as e:
        logger.error(f"‚ùå Error durante validaci√≥n: {str(e)}")
        return 1


if __name__ == "__main__":
    exit(main())
